{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ix-objectdetect-tut1-02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw2Y091Vc-Lo",
        "colab_type": "text"
      },
      "source": [
        "# Trainingsdaten Erstellen \n",
        "\n",
        "Im Folgenden werden die Rohdaten aufbereitet. Dazu werden die Bilder verkleinert und mit Hilfe von Data Augmentation variiert. Danach werden die mit LabelImg erstellten Annotationen in ein Trainings- und ein Testdatenset umgewandelt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEigv3nc53B",
        "colab_type": "code",
        "outputId": "2f5bde41-e521-48fa-fea8-5f40f0ec068e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-81dnal3g\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-81dnal3g\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 52.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (46.0.0)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=65100 sha256=dc8980be060f98e441e06ea1f519b66956be7b321750559282ba069c75a5f7bc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ty30plvp/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=076dbbea32be9a0f6e8cd7501f36fdc0df4b6bb5a6968bce63cf3d70f4c6dcd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.5 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjpBOh9UeHvE",
        "colab_type": "text"
      },
      "source": [
        "## Benötigte Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoHbl6lbeLjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, HTML \n",
        "\n",
        "from albumentations import *\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27BC6HHJeYKu",
        "colab_type": "text"
      },
      "source": [
        "## Rohdaten mouten\n",
        "\n",
        "Am einfachsten ist es, wenn man die Daten auf einem Cloud-Storage gespeichert hat. Für das Tutorial kommt Google Drive zum Einsatz. Auf diesem sind alle Bilder im Rohformat im Verzeichnis *ix-tut-raw* gespeichert. In die virtuelle Maschine von Colab lässt sich dieses Verzeichnis über einen einfachen **mount** Befehl freigeben mit "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS_JJ4VaecBA",
        "colab_type": "code",
        "outputId": "87bb5f4b-e3af-4fa1-a9d8-d28850809fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrFWIepzlLAZ",
        "colab_type": "text"
      },
      "source": [
        "Der Inhalt des so gemounteten Google Drive Verzeichnisses lässt sich mit **ls** einfach wie ein lokales Verzeichnis ausgeben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlWdfyOzeq-o",
        "colab_type": "code",
        "outputId": "e4acdc88-4526-48c6-d28e-e62f1ff2b21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "!ls -la 'drive/My Drive/data/ix-tut-raw' "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 100679\n",
            "-rw------- 1 root root 2234786 Mar 27 10:13 c3po_0001.jpg\n",
            "-rw------- 1 root root 2111258 Mar 27 10:13 c3po_0002.jpg\n",
            "-rw------- 1 root root 2150837 Mar 27 10:14 c3po_0003.jpg\n",
            "-rw------- 1 root root 2014113 Mar 27 10:14 c3po_0004.jpg\n",
            "-rw------- 1 root root 2122045 Mar 27 10:14 c3po_0005.jpg\n",
            "-rw------- 1 root root 2203209 Mar 27 10:14 c3po_0006.jpg\n",
            "-rw------- 1 root root 2166206 Mar 27 10:15 c3po_0007.jpg\n",
            "-rw------- 1 root root 2079133 Mar 27 10:15 c3po_0008.jpg\n",
            "-rw------- 1 root root 2121983 Mar 27 10:15 c3po_0009.jpg\n",
            "-rw------- 1 root root 2172382 Mar 27 10:15 c3po_0010.jpg\n",
            "-rw------- 1 root root 2391137 Mar 27 10:05 luke-skywalker_0001.jpg\n",
            "-rw------- 1 root root 2002863 Mar 27 10:05 luke-skywalker_0002.jpg\n",
            "-rw------- 1 root root 2053317 Mar 27 10:06 luke-skywalker_0003.jpg\n",
            "-rw------- 1 root root 1907511 Mar 27 10:06 luke-skywalker_0004.jpg\n",
            "-rw------- 1 root root 1838716 Mar 27 10:06 luke-skywalker_0005.jpg\n",
            "-rw------- 1 root root 1977107 Mar 27 10:06 luke-skywalker_0006.jpg\n",
            "-rw------- 1 root root 2196622 Mar 27 10:07 luke-skywalker_0007.jpg\n",
            "-rw------- 1 root root 1861247 Mar 27 10:07 luke-skywalker_0008.jpg\n",
            "-rw------- 1 root root 1918049 Mar 27 10:07 luke-skywalker_0009.jpg\n",
            "-rw------- 1 root root 2019980 Mar 27 10:07 luke-skywalker_0010.jpg\n",
            "-rw------- 1 root root 2147121 Mar 27 10:19 obi-wan-kinobi_0001.jpg\n",
            "-rw------- 1 root root 2102663 Mar 27 10:19 obi-wan-kinobi_0002.jpg\n",
            "-rw------- 1 root root 2055321 Mar 27 10:19 obi-wan-kinobi_0003.jpg\n",
            "-rw------- 1 root root 2098697 Mar 27 10:20 obi-wan-kinobi_0004.jpg\n",
            "-rw------- 1 root root 2183126 Mar 27 10:20 obi-wan-kinobi_0005.jpg\n",
            "-rw------- 1 root root 2077066 Mar 27 10:20 obi-wan-kinobi_0006.jpg\n",
            "-rw------- 1 root root 2208971 Mar 27 10:21 obi-wan-kinobi_0007.jpg\n",
            "-rw------- 1 root root 2191238 Mar 27 10:21 obi-wan-kinobi_0008.jpg\n",
            "-rw------- 1 root root 2168530 Mar 27 10:21 obi-wan-kinobi_0009.jpg\n",
            "-rw------- 1 root root 2035672 Mar 27 10:21 obi-wan-kinobi_0010.jpg\n",
            "-rw------- 1 root root 1931892 Mar 27 10:08 r2d2_0001.jpg\n",
            "-rw------- 1 root root 2082590 Mar 27 10:09 r2d2_0002.jpg\n",
            "-rw------- 1 root root 2081853 Mar 27 10:09 r2d2_0003.jpg\n",
            "-rw------- 1 root root 1983747 Mar 27 10:09 r2d2_0004.jpg\n",
            "-rw------- 1 root root 2026562 Mar 27 10:10 r2d2_0005.jpg\n",
            "-rw------- 1 root root 2016800 Mar 27 10:10 r2d2_0006.jpg\n",
            "-rw------- 1 root root 1902647 Mar 27 10:10 r2d2_0007.jpg\n",
            "-rw------- 1 root root 1982844 Mar 27 10:10 r2d2_0008.jpg\n",
            "-rw------- 1 root root 1973381 Mar 27 10:11 r2d2_0009.jpg\n",
            "-rw------- 1 root root 1938357 Mar 27 10:11 r2d2_0010.jpg\n",
            "-rw------- 1 root root 2055861 Mar 27 10:16 sturmtruppler_0001.jpg\n",
            "-rw------- 1 root root 1990125 Mar 27 10:16 sturmtruppler_0002.jpg\n",
            "-rw------- 1 root root 2101858 Mar 27 10:17 sturmtruppler_0003.jpg\n",
            "-rw------- 1 root root 2092167 Mar 27 10:17 sturmtruppler_0004.jpg\n",
            "-rw------- 1 root root 2031718 Mar 27 10:17 sturmtruppler_0005.jpg\n",
            "-rw------- 1 root root 2053385 Mar 27 10:17 sturmtruppler_0006.jpg\n",
            "-rw------- 1 root root 2020532 Mar 27 10:18 sturmtruppler_0007.jpg\n",
            "-rw------- 1 root root 2038072 Mar 27 10:18 sturmtruppler_0008.jpg\n",
            "-rw------- 1 root root 1976841 Mar 27 10:18 sturmtruppler_0009.jpg\n",
            "-rw------- 1 root root 1989871 Mar 27 10:18 sturmtruppler_0010.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TCLL9iLeL3y",
        "colab_type": "text"
      },
      "source": [
        "## Variablen\n",
        "\n",
        "Definition der Eingabe- und Ausgabeverzeichnisse. Das Ausgabeverzeichnis liegt auch wieder auf dem Google Drive. Zu beachten ist bei der Verwendung von Cloud Storage, dass die Synchronisation der Daten etwas dauern kann."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B73SH8_hePPI",
        "colab_type": "code",
        "outputId": "bf6d8f35-a3e6-410b-aa9d-653985c1d113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Eigene Fotos\n",
        "raw_images = '/content/drive/My Drive/data/ix-tut-raw/'\n",
        "\n",
        "# Ausgabeverzeichnis\n",
        "processed_images = '/content/drive/My Drive/data/ix-tut-processed/'\n",
        "\n",
        "# Alle Bilddateien mit der Endung jpg\n",
        "raw_image_lst = glob.glob(raw_images+'*.jpg')\n",
        "\n",
        "print(f\"Liste enthält {len(raw_image_lst)} Bilder\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Liste enthält 50 Bilder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuhl1zE9gYKw",
        "colab_type": "text"
      },
      "source": [
        "## Funktionen\n",
        "\n",
        "Ein paad Hilfsfunktionen für die Bildverarbeitung"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39JluT_-gZuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_img(img, figsize=(8, 8)):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.grid(False)\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.imshow(img)\n",
        "    plt.imshow(img)\n",
        "\n",
        "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
        "    dim = None\n",
        "    (h, w) = image.shape[:2]\n",
        "\n",
        "    if width is None and height is None:\n",
        "        return image\n",
        "\n",
        "    if width is None:\n",
        "        r = height / float(h)\n",
        "        dim = (int(w * r), height)\n",
        "\n",
        "    else:\n",
        "        r = width / float(w)\n",
        "        dim = (width, int(h * r))\n",
        "\n",
        "    resized = cv2.resize(image, dim, interpolation = inter)\n",
        "\n",
        "    return resized\n",
        "\n",
        "def processed_filename(image_filename):\n",
        "    filepath, filename_ext = os.path.split(image_filename)\n",
        "    filename, file_ext = os.path.splitext(filename_ext)\n",
        "    image_number = filename.split(\"_\")[1]\n",
        "\n",
        "    return filename, image_number, file_ext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBlmjBTCf0N2",
        "colab_type": "text"
      },
      "source": [
        "## Alle Bilder modifizieren\n",
        "\n",
        "Die Ergebnisbilder heißen wie das Originalbild plus einer fortlaufenden Nummer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWQCylJih97I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename, image_number, file_ext = processed_filename(raw_image_lst[0])\n",
        "print(filename)\n",
        "print(image_number)\n",
        "print(file_ext)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik41HsrTm4Y4",
        "colab_type": "text"
      },
      "source": [
        "## Schleife über alle Bilder\n",
        "\n",
        "In diesem Schritt wird die ursprüngliche Bildgröße reduziert und das Bild wird in das für das YOLOv3 neuronale Netz optimierte Format 416x416 (13*32) gebracht. Pro Bild werden dann 13 unterschiedliche Bildverarbeitungsschritte angewendet um die nötigen Bildvariationen zu erzeugen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smGBluH8f7ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = 0\n",
        "idx = 0\n",
        "proc_imgs_dict = {}\n",
        "show = True\n",
        "\n",
        "for image in raw_image_lst:\n",
        "  idx = idx + 1\n",
        "  print(f\"Verarbeite {idx}/{len(raw_image_lst)} Bild '{image}...\")\n",
        "  \n",
        "  # Bild einlesen\n",
        "  img = cv2.imread(image)\n",
        "  \n",
        "  # Data Augmentation 1: Größenänderung (416x416) \n",
        "  img1 = image_resize(img, height=416)\n",
        "  aug = CenterCrop(416,416, p=1.0)\n",
        "  img1 = aug.apply(img1)\n",
        "\n",
        "  aug_name = \"resize\"\n",
        "  processed_image_filename, image_number, file_ext = processed_filename(image)\n",
        "  proc_filename = f\"{processed_images+processed_image_filename}_{aug_name}{file_ext}\"\n",
        "  print(f\"Store {proc_filename}\")\n",
        "  cv2.imwrite(proc_filename, img1)\n",
        "  proc_imgs_dict[aug_name] = img1\n",
        "  images = images + 1\n",
        "\n",
        "  # Data Augmentation 2: Vertikale Spiegerlung\n",
        "  idx = idx + 1\n",
        "  aug = VerticalFlip(p=1)\n",
        "  aug_name = \"vertflip\"\n",
        "  img2 = aug.apply(img1)\n",
        "  proc_filename = f\"{processed_images+processed_image_filename}_{aug_name}{file_ext}\" \n",
        "  print(f\"Store {proc_filename}\")\n",
        "  cv2.imwrite(proc_filename, img2)\n",
        "  proc_imgs_dict[aug_name] = img2\n",
        "  images = images + 1\n",
        "\n",
        "  # Data Augmentation 3: Horizontale Spiegelung\n",
        "  idx = idx + 1\n",
        "  aug = HorizontalFlip(p=1)\n",
        "  aug_name = \"horzflip\"\n",
        "  img3 = aug.apply(img1)\n",
        "  proc_filename = f\"{processed_images+processed_image_filename}_{aug_name}{file_ext}\"\n",
        "  print(f\"Store {proc_filename}\")\n",
        "  cv2.imwrite(proc_filename, img3)\n",
        "  proc_imgs_dict[aug_name] = img3\n",
        "  images = images + 1\n",
        "\n",
        "  # Data Augmentation 4: Elatische Transformation\n",
        "  idx = idx + 1\n",
        "  aug = ElasticTransform(alpha=101, sigma=81, alpha_affine=53, p=0.5)\n",
        "  aug_name = \"elastic\"\n",
        "  img4 = aug.apply(img1)\n",
        "  proc_filename = f\"{processed_images+processed_image_filename}_{aug_name}{file_ext}\"\n",
        "  print(f\"Store {proc_filename}\")\n",
        "  cv2.imwrite(proc_filename, img4)\n",
        "  proc_imgs_dict[aug_name] = img4\n",
        "  images = images + 1\n",
        "\n",
        "  # Data Augmentation 5: Rotation\n",
        "  idx = idx + 1\n",
        "  aug = RandomRotate90(p=1.0)\n",
        "  aug_name = \"rot\"\n",
        "  img5 = aug.apply(img1, factor=45)\n",
        "  proc_filename = f\"{processed_images+processed_image_filename}_{aug_name}{file_ext}\"\n",
        "  print(f\"Store {proc_filename}\")\n",
        "  cv2.imwrite(proc_filename, img5)\n",
        "  proc_imgs_dict[aug_name] = img5\n",
        "  images = images + 1\n",
        "\n",
        "  for proc_name in proc_imgs_dict:\n",
        "    idx = 0\n",
        "    \n",
        "    # Data Augmentation 6: Alphawert ändern\n",
        "    idx = idx + 1\n",
        "    alpha = 2.2\n",
        "    aug = RandomContrast(limit=0.9, p=1.0)\n",
        "    aug_name = \"alpha\"\n",
        "    img6 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img6)\n",
        "    images = images + 1\n",
        "\n",
        "    # Data Augmentation 7: Zufällige Helligkeit\n",
        "    idx = idx + 1\n",
        "    alpha = 1.2\n",
        "    aug = RandomBrightness(limit=0.2, p=1.0)\n",
        "    aug_name = \"bight\"\n",
        "    img7 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img7)\n",
        "    images = images + 1\n",
        "    \n",
        "    # Data Augmentation 8: Grauwerte\n",
        "    idx = idx + 1\n",
        "    aug = ToGray(p=0.5)\n",
        "    aug_name = \"gray\"\n",
        "    img8 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img8)\n",
        "    images = images + 1\n",
        "\n",
        "    # Data Augmentation 9: Sepia\n",
        "    idx = idx + 1\n",
        "    aug = ToSepia(p=1)\n",
        "    aug_name = \"sepia\"\n",
        "    img9 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img9)\n",
        "    images = images + 1\n",
        "\n",
        "    # Data Augmentation 10: Weichzeichner\n",
        "    idx = idx + 1\n",
        "    aug = Blur(blur_limit=11, p=1)\n",
        "    aug_name = \"blur\"\n",
        "    img10 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img10)\n",
        "    images = images + 1\n",
        "\n",
        "    # Data Augmentation 11: Noise\n",
        "    idx = idx + 1\n",
        "    aug = MultiplicativeNoise(always_apply=True, elementwise=True, multiplier=(0.9, 1.1), p=1.0)\n",
        "    aug_name = \"noise\"\n",
        "    img11 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img11)\n",
        "    images = images + 1\n",
        "\n",
        "    # Data Augmentation 12: JPEGCompression\n",
        "    idx = idx + 1\n",
        "    aug = JpegCompression(quality_lower=0, quality_upper=1, p=1)\n",
        "    aug_name = \"jpeg\"\n",
        "    img12 = aug.apply(proc_imgs_dict[proc_name], alpha=alpha)\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img12)\n",
        "    images = images + 1\n",
        "\n",
        "    # Data Augmentation 13: Farbkanal\n",
        "    idx = idx + 1\n",
        "    aug = ChannelDropout(channel_drop_range=(1, 1), fill_value=0, p=1)\n",
        "    aug_name = \"channel\"\n",
        "    img13 = aug.apply(proc_imgs_dict[proc_name])\n",
        "    proc_filename = f\"{processed_images+processed_image_filename}_{proc_name}_{aug_name}{file_ext}\"\n",
        "    print(f\"Store {proc_filename}\")\n",
        "    cv2.imwrite(proc_filename, img13)\n",
        "    images = images + 1\n",
        "    \n",
        "  # Bilderindex zurücksetzen\n",
        "  idx = 0\n",
        "\n",
        "print(\"{} Bilder erzeugt\".format(images))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbOF1lpSojU3",
        "colab_type": "text"
      },
      "source": [
        "Es sollten sich jetzt 2250 Bildateien vorhanden sein. Für die 5 LEGO-Figuren wurden jeweils 10 Oiginalbilder mit 9 Variationen erstellt. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1AMkys0yuTG",
        "colab_type": "code",
        "outputId": "5ddedc79-ddd3-4319-8ca7-9fe0ef9a227b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls -l \"/content/drive/My Drive/data/ix-tut-processed/\" | wc -l"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJa9OqYGpfzL",
        "colab_type": "text"
      },
      "source": [
        "## Bounding Boxes erstellen\n",
        "\n",
        "Im nächsten Schritt müssen die Bilder annotiert werden. Da nur die fünf verschiedenen Transformationen resize, vertflip, horzflip, elatic und rot Koordinaten verändern, müssen nur diese mit Bounding Boxen versehen werden. Die anderen Annotation-Dateien erzeugen wir im Anschluß durch einfaches Kopieren.\n",
        "\n",
        "Die Annotationen werden mit Hilfe von LabelImg erzeugt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0DH2-TwLtHS",
        "colab_type": "text"
      },
      "source": [
        "## Bounding Box Dateien kopieren"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVEWyOeDLzdQ",
        "colab_type": "code",
        "outputId": "e248326f-e932-4236-bb50-6ffc0918bfdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "processed_images = '/content/drive/My Drive/data/ix-tut-annotations/'\n",
        "annotated_variations = '/content/drive/My Drive/data/ix-tut-annotations-all/'\n",
        "resize_processed_image_lst = glob.glob(processed_images+'*resize.xml')\n",
        "vertflip_processed_image_lst = glob.glob(processed_images+'*vertflip.xml')\n",
        "horzflip_processed_image_lst = glob.glob(processed_images+'*horzflip.xml')\n",
        "elastic_processed_image_lst = glob.glob(processed_images+'*elastic.xml')\n",
        "rot_processed_image_lst = glob.glob(processed_images+'*rot.xml')\n",
        "\n",
        "all_processed = resize_processed_image_lst + vertflip_processed_image_lst + horzflip_processed_image_lst + elastic_processed_image_lst + rot_processed_image_lst\n",
        "print(f\"{len(all_processed)} to be work on\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250 to be work on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOcNSmg_Eq8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "variation_lst = [\"alpha\",\"bright\",\"blur\",\"channel\",\"gray\",\"jpeg\",\"noise\",\"sepia\"] \n",
        "idx = 0\n",
        "for process in all_processed:\n",
        "  filename, image_number, file_ext = processed_filename(process)\n",
        "  old_annotation_filename = annotated_variations + filename + file_ext\n",
        "  print(f\"Kopiere {process} nach {old_annotation_filename}\") \n",
        "  copyfile(process, old_annotation_filename)\n",
        "  idx = idx + 1\n",
        "  for variation in variation_lst:\n",
        "    new_annotation_filename = annotated_variations + filename + \"_\" + variation + file_ext\n",
        "    print(f\"Kopiere {process} nach {new_annotation_filename}\")\n",
        "    copyfile(process, new_annotation_filename)\n",
        "    idx = idx + 1\n",
        "\n",
        "print(f\"{idx} Dateien erzeugt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R3AsNLLm0cK",
        "colab_type": "text"
      },
      "source": [
        "## YOLOv3 Anntotation Datei schreiben"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BrSMqHkm4v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "classes = [\"r2d2\",\"c3po\",\"luke-skywalker\",\"obi-wan-kinobi\",\"sturmtruppler\"]\n",
        "\n",
        "def convert_voc_annotation(image_dir, voc_filename):\n",
        "    in_file = open(voc_filename)\n",
        "    tree=ET.parse(in_file)\n",
        "    root = tree.getroot()\n",
        "    yolo_line = \"\"\n",
        "\n",
        "\n",
        "    #image_path = root.find('path').text\n",
        "    #image_filename = os.path.basename(image_path)\n",
        "\n",
        "    image_filename, image_number, file_ext = processed_filename(voc_filename)\n",
        "    new_image_filename = image_dir + '/' + image_filename + '.jpg'\n",
        "\n",
        "    for obj in root.iter('object'):\n",
        "        difficult = obj.find('difficult').text\n",
        "        cls = obj.find('name').text\n",
        "        if cls not in classes or int(difficult)==1:\n",
        "            continue\n",
        "        cls_id = classes.index(cls)\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
        "        yolo_line = new_image_filename + \" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id)\n",
        "        #yolo_line = image_path + \" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id)\n",
        "\n",
        "    return yolo_line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTQK9OEOnBdX",
        "colab_type": "code",
        "outputId": "959ec7cc-f2b0-47bc-a59f-49237133b63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_files = []\n",
        "annotation_path = '/content/drive/My Drive/data/ix-tut-annotations-all'\n",
        "image_path = '/content/drive/My Drive/data/My Drive/data/ix-tut-processed'\n",
        "\n",
        "voc_filenames = glob.glob(annotation_path+'/*.xml')\n",
        "voc_filenames.sort()\n",
        "\n",
        "print(f\"Found {len(voc_filenames)} files...\")\n",
        "\n",
        "for voc_filename in voc_filenames:\n",
        "    yolo_line = convert_voc_annotation(image_path, voc_filename)\n",
        "    #print(yolo_line)\n",
        "    all_files.append(yolo_line)\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2250 files...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntJ_OUHz-hYJ",
        "colab_type": "text"
      },
      "source": [
        "## Daten in Training und Test aufteilen \n",
        "\n",
        "Teilen der Daten in einen Trainings- und einen Testanteil im Verhältnis 80:20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KpZiFz0vUOx",
        "colab_type": "code",
        "outputId": "8b22b032-dabc-441b-f7e1-def14fb75991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_files, test_files, _, _ = train_test_split(all_files, all_files, test_size = 0.15, random_state = 0)\n",
        "\n",
        "print(f\"Anzahl Testdateien : {len(test_files)}\")\n",
        "print(f\"Anzahl Trainingsdateien : {len(train_files)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anzahl Testdateien : 338\n",
            "Anzahl Trainingsdateien : 1912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gydxIwmW-vxq",
        "colab_type": "text"
      },
      "source": [
        "## Trainings und Test-Datensätze erzeugen\n",
        "\n",
        "Die beiden Teile werden in zwei getrennte Dateien abgespeichert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhhkQ-JQvrW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = open('starwars_train.txt', 'w')\n",
        "test_file  = open('starwars_test.txt', 'w')\n",
        "\n",
        "for imagefile_and_box in train_files: \n",
        "   train_file.write(imagefile_and_box)\n",
        "   train_file.write('\\n')\n",
        "\n",
        "train_file.close()\n",
        "\n",
        "for imagefile_and_box in test_files: \n",
        "   test_file.write(imagefile_and_box)\n",
        "   test_file.write('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO5wB9TYwpkA",
        "colab_type": "code",
        "outputId": "804bc853-8d93-4181-f325-77eec5892d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 264\n",
            "drwxr-xr-x 1 root root   4096 Mar 29 14:59 .\n",
            "drwxr-xr-x 1 root root   4096 Mar 29 14:43 ..\n",
            "drwxr-xr-x 1 root root   4096 Mar 25 16:11 .config\n",
            "drwx------ 4 root root   4096 Mar 29 14:44 drive\n",
            "drwxr-xr-x 1 root root   4096 Mar 18 16:23 sample_data\n",
            "-rw-r--r-- 1 root root  32952 Mar 29 14:59 starwars_test.txt\n",
            "-rw-r--r-- 1 root root 209248 Mar 29 14:59 starwars_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Uy9em5xSvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('starwars_train.txt') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x17iyozf_Onq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('starwars_test.txt') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}